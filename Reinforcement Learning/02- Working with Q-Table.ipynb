{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "948637e7",
   "metadata": {},
   "source": [
    "**Q-Learning** is about updating `Q-values` in the `Q-Table` to maximize the reward about going to a state for a particular step for an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8231d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # toolkit for RL algo\n",
    "import numpy as np # mathematical manipulation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38e62d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the environment\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8804365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning settings\n",
    "LEARNING_RATE = 0.1 # rate at which agent learns or updates Q-values\n",
    "DISCOUNT = 0.95 # the factor of max reward to be considered after the step is taken\n",
    "EPISODES = 25000 # number of iterations to run\n",
    "SHOW_EVERY = 2000 # every number of episodes environment is rendered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "244cd4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 20]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# discrete observation space size, for combinations of observation space samples\n",
    "DISCRETE_OS_SIZE = [20] * len(env.observation_space.low)\n",
    "DISCRETE_OS_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a65a11e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09 , 0.007])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# window size of each observation smaple\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
    "discrete_os_win_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30fd3aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 20, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DISCRETE_OS_SIZE + [env.action_space.n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b109ba7",
   "metadata": {},
   "source": [
    "As seen from above, here `q-table` can be thought of `20x20x3`, where `20x20` is the every possible combination of observation space samples(position,velocity) and `x3` can be thought of the action-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b02933c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.69050913, -1.16948244, -1.26313923],\n",
       "        [-0.90808084, -1.66328272, -0.31475987],\n",
       "        [-0.40395714, -1.870182  , -0.67626117],\n",
       "        ...,\n",
       "        [-1.84295164, -1.35335332, -0.83033135],\n",
       "        [-0.16397614, -0.6097854 , -1.29303367],\n",
       "        [-1.37375663, -1.31751277, -1.92931083]],\n",
       "\n",
       "       [[-0.00791838, -1.77304587, -0.32802657],\n",
       "        [-1.05350572, -1.56665497, -1.37713454],\n",
       "        [-0.22860324, -1.22899994, -1.62200979],\n",
       "        ...,\n",
       "        [-1.78616919, -1.28981191, -1.40489348],\n",
       "        [-0.50521477, -1.8132981 , -1.68508131],\n",
       "        [-1.77373601, -1.57999813, -0.89155134]],\n",
       "\n",
       "       [[-1.72793273, -0.91497616, -0.43850907],\n",
       "        [-0.54355632, -1.69615882, -1.05096149],\n",
       "        [-1.54429143, -0.59016985, -1.48673438],\n",
       "        ...,\n",
       "        [-0.54433672, -0.89091968, -0.24599144],\n",
       "        [-1.66048   , -0.26567434, -1.85599074],\n",
       "        [-1.00906792, -1.48288185, -0.96911025]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.20216727, -0.26740148, -0.62363353],\n",
       "        [-1.33317598, -0.66801667, -1.94296638],\n",
       "        [-1.61420208, -1.84487749, -0.65857306],\n",
       "        ...,\n",
       "        [-1.54959297, -1.56765894, -0.72664396],\n",
       "        [-0.16811491, -1.5911935 , -0.74863052],\n",
       "        [-1.14196755, -0.0288006 , -0.14939858]],\n",
       "\n",
       "       [[-1.78475486, -1.28282522, -0.96233652],\n",
       "        [-0.71339388, -1.72829016, -1.19337845],\n",
       "        [-0.80800074, -1.05000876, -1.20979838],\n",
       "        ...,\n",
       "        [-0.04502631, -0.60689493, -0.6158038 ],\n",
       "        [-1.99670227, -1.02573472, -1.5341334 ],\n",
       "        [-0.34087243, -1.17197886, -1.32799112]],\n",
       "\n",
       "       [[-0.25979557, -1.64853634, -0.69851591],\n",
       "        [-1.58132754, -0.19016795, -1.45157464],\n",
       "        [-1.57998503, -1.31958928, -0.98940719],\n",
       "        ...,\n",
       "        [-0.06523251, -0.70133569, -0.17333613],\n",
       "        [-0.59457149, -1.47552087, -0.33840401],\n",
       "        [-0.54325069, -0.861097  , -1.45549014]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise q-table with randomo q-values\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36d36505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of q-table\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ec727",
   "metadata": {},
   "source": [
    "Q-Learning is based on **exploration-exploitation trade-off**, where the agent tries to `explore` more possible new states by taking an action, but also `exploit` for maximising the reward for accomplishment of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6eef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    '''\n",
    "    Converts the granular state of an agent to more\n",
    "    discrete values, by considering the discrete window size\n",
    "    '''\n",
    "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(np.int32))  # we use this tuple to look up the 3 Q values for the available actions in the q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ceb91c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.000640051204096e-05"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploration settings - random moves\n",
    "epsilon = 1  # not a constant, qoing to be decayed\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES//2\n",
    "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "epsilon_decay_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73e64e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stats for episodes\n",
    "ep_rewards = []\n",
    "aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []}\n",
    "STATS_EVERY = 100 # episodes after which to record stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7dd2df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial discrete state of the agent\n",
    "discrete_state = get_discrete_state(env.reset())\n",
    "discrete_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b797117",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b51431e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Episode:     0, average reward: -2.0, current epsilon: 1.00\n",
      "Episode:   100, average reward:  0.0, current epsilon: 0.99\n",
      "Episode:   200, average reward:  0.0, current epsilon: 0.98\n",
      "Episode:   300, average reward:  0.0, current epsilon: 0.98\n",
      "Episode:   400, average reward:  0.0, current epsilon: 0.97\n",
      "Episode:   500, average reward:  0.0, current epsilon: 0.96\n",
      "Episode:   600, average reward:  0.0, current epsilon: 0.95\n",
      "Episode:   700, average reward:  0.0, current epsilon: 0.94\n",
      "Episode:   800, average reward:  0.0, current epsilon: 0.94\n",
      "Episode:   900, average reward:  0.0, current epsilon: 0.93\n",
      "Episode:  1000, average reward:  0.0, current epsilon: 0.92\n",
      "Episode:  1100, average reward:  0.0, current epsilon: 0.91\n",
      "Episode:  1200, average reward:  0.0, current epsilon: 0.90\n",
      "Episode:  1300, average reward:  0.0, current epsilon: 0.90\n",
      "Episode:  1400, average reward:  0.0, current epsilon: 0.89\n",
      "Episode:  1500, average reward:  0.0, current epsilon: 0.88\n",
      "Episode:  1600, average reward:  0.0, current epsilon: 0.87\n",
      "Episode:  1700, average reward:  0.0, current epsilon: 0.86\n",
      "Episode:  1800, average reward:  0.0, current epsilon: 0.86\n",
      "Episode:  1900, average reward:  0.0, current epsilon: 0.85\n",
      "2000\n",
      "Episode:  2000, average reward:  0.0, current epsilon: 0.84\n",
      "Episode:  2100, average reward:  0.0, current epsilon: 0.83\n",
      "Episode:  2200, average reward:  0.0, current epsilon: 0.82\n",
      "Episode:  2300, average reward:  0.0, current epsilon: 0.82\n",
      "Episode:  2400, average reward:  0.0, current epsilon: 0.81\n",
      "Episode:  2500, average reward:  0.0, current epsilon: 0.80\n",
      "Episode:  2600, average reward:  0.0, current epsilon: 0.79\n",
      "Episode:  2700, average reward:  0.0, current epsilon: 0.78\n",
      "Episode:  2800, average reward:  0.0, current epsilon: 0.78\n",
      "Episode:  2900, average reward:  0.0, current epsilon: 0.77\n",
      "Episode:  3000, average reward:  0.0, current epsilon: 0.76\n",
      "Episode:  3100, average reward:  0.0, current epsilon: 0.75\n",
      "Episode:  3200, average reward:  0.0, current epsilon: 0.74\n",
      "Episode:  3300, average reward:  0.0, current epsilon: 0.74\n",
      "Episode:  3400, average reward:  0.0, current epsilon: 0.73\n",
      "Episode:  3500, average reward:  0.0, current epsilon: 0.72\n",
      "Episode:  3600, average reward:  0.0, current epsilon: 0.71\n",
      "Episode:  3700, average reward:  0.0, current epsilon: 0.70\n",
      "Episode:  3800, average reward:  0.0, current epsilon: 0.70\n",
      "Episode:  3900, average reward:  0.0, current epsilon: 0.69\n",
      "4000\n",
      "Episode:  4000, average reward:  0.0, current epsilon: 0.68\n",
      "Episode:  4100, average reward:  0.0, current epsilon: 0.67\n",
      "Episode:  4200, average reward:  0.0, current epsilon: 0.66\n",
      "Episode:  4300, average reward:  0.0, current epsilon: 0.66\n",
      "Episode:  4400, average reward:  0.0, current epsilon: 0.65\n",
      "Episode:  4500, average reward:  0.0, current epsilon: 0.64\n",
      "Episode:  4600, average reward:  0.0, current epsilon: 0.63\n",
      "Episode:  4700, average reward:  0.0, current epsilon: 0.62\n",
      "Episode:  4800, average reward:  0.0, current epsilon: 0.62\n",
      "Episode:  4900, average reward:  0.0, current epsilon: 0.61\n",
      "Episode:  5000, average reward:  0.0, current epsilon: 0.60\n",
      "Episode:  5100, average reward:  0.0, current epsilon: 0.59\n",
      "Episode:  5200, average reward:  0.0, current epsilon: 0.58\n",
      "Episode:  5300, average reward:  0.0, current epsilon: 0.58\n",
      "Episode:  5400, average reward:  0.0, current epsilon: 0.57\n",
      "Episode:  5500, average reward:  0.0, current epsilon: 0.56\n",
      "Episode:  5600, average reward:  0.0, current epsilon: 0.55\n",
      "Episode:  5700, average reward:  0.0, current epsilon: 0.54\n",
      "Episode:  5800, average reward:  0.0, current epsilon: 0.54\n",
      "Episode:  5900, average reward:  0.0, current epsilon: 0.53\n",
      "6000\n",
      "Episode:  6000, average reward:  0.0, current epsilon: 0.52\n",
      "Episode:  6100, average reward:  0.0, current epsilon: 0.51\n",
      "Episode:  6200, average reward:  0.0, current epsilon: 0.50\n",
      "Episode:  6300, average reward:  0.0, current epsilon: 0.50\n",
      "Episode:  6400, average reward:  0.0, current epsilon: 0.49\n",
      "Episode:  6500, average reward:  0.0, current epsilon: 0.48\n",
      "Episode:  6600, average reward:  0.0, current epsilon: 0.47\n",
      "Episode:  6700, average reward:  0.0, current epsilon: 0.46\n",
      "Episode:  6800, average reward:  0.0, current epsilon: 0.46\n",
      "Episode:  6900, average reward:  0.0, current epsilon: 0.45\n",
      "Episode:  7000, average reward:  0.0, current epsilon: 0.44\n",
      "Episode:  7100, average reward:  0.0, current epsilon: 0.43\n",
      "Episode:  7200, average reward:  0.0, current epsilon: 0.42\n",
      "Episode:  7300, average reward:  0.0, current epsilon: 0.42\n",
      "Episode:  7400, average reward:  0.0, current epsilon: 0.41\n",
      "Episode:  7500, average reward:  0.0, current epsilon: 0.40\n",
      "Episode:  7600, average reward:  0.0, current epsilon: 0.39\n",
      "Episode:  7700, average reward:  0.0, current epsilon: 0.38\n",
      "Episode:  7800, average reward:  0.0, current epsilon: 0.38\n",
      "Episode:  7900, average reward:  0.0, current epsilon: 0.37\n",
      "8000\n",
      "Episode:  8000, average reward:  0.0, current epsilon: 0.36\n",
      "Episode:  8100, average reward:  0.0, current epsilon: 0.35\n",
      "Episode:  8200, average reward:  0.0, current epsilon: 0.34\n",
      "Episode:  8300, average reward:  0.0, current epsilon: 0.34\n",
      "Episode:  8400, average reward:  0.0, current epsilon: 0.33\n",
      "Episode:  8500, average reward:  0.0, current epsilon: 0.32\n",
      "Episode:  8600, average reward:  0.0, current epsilon: 0.31\n",
      "Episode:  8700, average reward:  0.0, current epsilon: 0.30\n",
      "Episode:  8800, average reward:  0.0, current epsilon: 0.30\n",
      "Episode:  8900, average reward:  0.0, current epsilon: 0.29\n",
      "Episode:  9000, average reward:  0.0, current epsilon: 0.28\n",
      "Episode:  9100, average reward:  0.0, current epsilon: 0.27\n",
      "Episode:  9200, average reward:  0.0, current epsilon: 0.26\n",
      "Episode:  9300, average reward:  0.0, current epsilon: 0.26\n",
      "Episode:  9400, average reward:  0.0, current epsilon: 0.25\n",
      "Episode:  9500, average reward:  0.0, current epsilon: 0.24\n",
      "Episode:  9600, average reward:  0.0, current epsilon: 0.23\n",
      "Episode:  9700, average reward:  0.0, current epsilon: 0.22\n",
      "Episode:  9800, average reward:  0.0, current epsilon: 0.22\n",
      "Episode:  9900, average reward:  0.0, current epsilon: 0.21\n",
      "10000\n",
      "Episode: 10000, average reward:  0.0, current epsilon: 0.20\n",
      "Episode: 10100, average reward:  0.0, current epsilon: 0.19\n",
      "Episode: 10200, average reward:  0.0, current epsilon: 0.18\n",
      "Episode: 10300, average reward:  0.0, current epsilon: 0.18\n",
      "Episode: 10400, average reward:  0.0, current epsilon: 0.17\n",
      "Episode: 10500, average reward:  0.0, current epsilon: 0.16\n",
      "Episode: 10600, average reward:  0.0, current epsilon: 0.15\n",
      "Episode: 10700, average reward:  0.0, current epsilon: 0.14\n",
      "Episode: 10800, average reward:  0.0, current epsilon: 0.14\n",
      "Episode: 10900, average reward:  0.0, current epsilon: 0.13\n",
      "Episode: 11000, average reward:  0.0, current epsilon: 0.12\n",
      "Episode: 11100, average reward:  0.0, current epsilon: 0.11\n",
      "Episode: 11200, average reward:  0.0, current epsilon: 0.10\n",
      "Episode: 11300, average reward:  0.0, current epsilon: 0.10\n",
      "Episode: 11400, average reward:  0.0, current epsilon: 0.09\n",
      "Episode: 11500, average reward:  0.0, current epsilon: 0.08\n",
      "Episode: 11600, average reward:  0.0, current epsilon: 0.07\n",
      "Episode: 11700, average reward:  0.0, current epsilon: 0.06\n",
      "Episode: 11800, average reward:  0.0, current epsilon: 0.06\n",
      "Episode: 11900, average reward:  0.0, current epsilon: 0.05\n",
      "12000\n",
      "Episode: 12000, average reward:  0.0, current epsilon: 0.04\n",
      "Episode: 12100, average reward:  0.0, current epsilon: 0.03\n",
      "Episode: 12200, average reward:  0.0, current epsilon: 0.02\n",
      "Episode: 12300, average reward:  0.0, current epsilon: 0.02\n",
      "Episode: 12400, average reward:  0.0, current epsilon: 0.01\n",
      "Episode: 12500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 12600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 12700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 12800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 12900, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 13900, average reward:  0.0, current epsilon: -0.00\n",
      "14000\n",
      "Episode: 14000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 14100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 14200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 14300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 14400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 14500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 14600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 14700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 14800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 14900, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 15900, average reward:  0.0, current epsilon: -0.00\n",
      "16000\n",
      "Episode: 16000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 16100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 16200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 16300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 16400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 16500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 16600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 16700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 16800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 16900, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 17900, average reward:  0.0, current epsilon: -0.00\n",
      "18000\n",
      "Episode: 18000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 18100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 18200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 18300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 18400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 18500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 18600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 18700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 18800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 18900, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 19900, average reward:  0.0, current epsilon: -0.00\n",
      "20000\n",
      "Episode: 20000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 20100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 20200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 20300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 20400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 20500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 20600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 20700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 20800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 20900, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 21900, average reward:  0.0, current epsilon: -0.00\n",
      "22000\n",
      "Episode: 22000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 22100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 22200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 22300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 22400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 22500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 22600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 22700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 22800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 22900, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 23900, average reward:  0.0, current epsilon: -0.00\n",
      "24000\n",
      "Episode: 24000, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 24100, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 24200, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 24300, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 24400, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 24500, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 24600, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 24700, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 24800, average reward:  0.0, current epsilon: -0.00\n",
      "Episode: 24900, average reward:  0.0, current epsilon: -0.00\n"
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES): # run through all the episodes\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    if episode % SHOW_EVERY == 0: # render the environment every SHOW_EVERY episodes \n",
    "        render = True\n",
    "        print(episode)\n",
    "    else:\n",
    "        render = False\n",
    "        \n",
    "    while not done: # run the agent till job is not done\n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table for max q-value\n",
    "            action = np.argmax(q_table[discrete_state]) # get the action with maximum q-value - exploitation\n",
    "        else:\n",
    "            # Get random action - exploration\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "            \n",
    "        new_state, reward, done, _ = env.step(action) # perform the step for that action\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        new_discrete_state = get_discrete_state(new_state) # get the new discrete state after performing the action\n",
    "        \n",
    "        if render: # render the environment\n",
    "            env.render()\n",
    "\n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        if not done:\n",
    "\n",
    "            # Maximum possible Q value in next step (for new state)\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            # Current Q value (for current state and performed action)\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            # And here's our equation for a new Q value for current state and action\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            # Update Q table with new Q value\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "\n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            # the reward at goal accomplishment is 0, because it is achieved\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        # make new discrete state the state for next iteration \n",
    "        discrete_state = new_discrete_state\n",
    "        \n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "    \n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % STATS_EVERY: # equiavalent to if episode % STATS_EVERY == 0:\n",
    "        average_reward = sum(ep_rewards[-STATS_EVERY:])/STATS_EVERY\n",
    "        aggr_ep_rewards['ep'].append(episode)\n",
    "        aggr_ep_rewards['avg'].append(average_reward)\n",
    "        aggr_ep_rewards['max'].append(max(ep_rewards[-STATS_EVERY:]))\n",
    "        aggr_ep_rewards['min'].append(min(ep_rewards[-STATS_EVERY:]))\n",
    "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19598206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1385f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
