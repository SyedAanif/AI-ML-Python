{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04- Deep Q-Learning/Networks(DQNs).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will try to coverthe basics around **Deep Q-Learning/Networks(DQNs)**.\n",
        "\n",
        "We will be making use of a **Convolutional Neural Network(CNN)** whose input will be the *environment* the *agent* operates on as an image and the ouput will be the corresponding **Q-values**.\n",
        "\n",
        "Here we focus on the *new Q-value* in terms of **REWARD + DISCOUNT x max_future_q**.\n",
        "\n",
        "**DQNs** help in learning a complex environment with minimum memory requirement as there is no need of storing values in a Q-Table, but this neural network takes lot of training time.\n",
        "\n",
        "We will be making use of the same custom environment of player, enemy and food."
      ],
      "metadata": {
        "id": "oJHdmEaiFwbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # array manipulation\n",
        "\n",
        "# Tensorflow Keras for Neural Network\n",
        "import tensorflow as tf\n",
        "\n",
        "# Q-value\n",
        "from collections import deque\n",
        "\n",
        "# pickling at time stamps\n",
        "import time\n",
        "\n",
        "# random numbers\n",
        "import random\n",
        "\n",
        "# bar progress of events\n",
        "from tqdm import tqdm\n",
        "\n",
        "# directory manipulation\n",
        "import os\n",
        "\n",
        "# input image and visualisation\n",
        "from PIL import Image\n",
        "import cv2"
      ],
      "metadata": {
        "id": "fI0wsRdOGnZD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DISCOUNT = 0.99 # factor/percentage of max future Q to be considered\n",
        "\n",
        "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
        "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
        "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
        "\n",
        "MODEL_NAME = '2x256'\n",
        "MIN_REWARD = -200  # For model save\n",
        "\n",
        "# Environment settings\n",
        "EPISODES = 20_000\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  # not a constant, going to be decayed\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001\n",
        "\n",
        "#  Stats settings\n",
        "AGGREGATE_STATS_EVERY = 50  # episodes\n",
        "SHOW_PREVIEW = False # render environment"
      ],
      "metadata": {
        "id": "fW8JVgO6HVPc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class to get objects of player, food, enemy\n",
        "class Blob:\n",
        "  def __init__(self, size):\n",
        "    '''\n",
        "    Used to initialise the objects of food, player, enemy at random locations\n",
        "    in the grid size.\n",
        "    '''\n",
        "    self.size = size\n",
        "    self.x = np.random.randint(0, size)\n",
        "    self.y = np.random.randint(0, size)\n",
        "\n",
        "  def __str__(self):\n",
        "    '''\n",
        "    Overloaded string method to display co-ordinates of an object\n",
        "    '''\n",
        "    return f\"Blob ({self.x}, {self.y})\"\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    '''\n",
        "    Overloaded subtraction method to get the relative position of one \n",
        "    object w.r.t other.\n",
        "    '''\n",
        "    return (self.x-other.x, self.y-other.y)\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    '''\n",
        "    Overloaded equality method to check if two objects are at the same co-ordinates\n",
        "    '''\n",
        "    return self.x == other.x and self.y == other.y\n",
        "\n",
        "  def action(self, choice):\n",
        "    '''\n",
        "    Gives us 9 total movement options. (0,1,2,3,4,5,6,7,8)\n",
        "    Diagonal, up, down, right, left, no movement.\n",
        "    '''\n",
        "    if choice == 0:\n",
        "      self.move(x=1, y=1)\n",
        "    elif choice == 1:\n",
        "      self.move(x=-1, y=-1)\n",
        "    elif choice == 2:\n",
        "      self.move(x=-1, y=1)\n",
        "    elif choice == 3:\n",
        "      self.move(x=1, y=-1)\n",
        "    elif choice == 4:\n",
        "      self.move(x=1, y=0)\n",
        "    elif choice == 5:\n",
        "      self.move(x=-1, y=0)\n",
        "    elif choice == 6:\n",
        "      self.move(x=0, y=1)\n",
        "    elif choice == 7:\n",
        "      self.move(x=0, y=-1)\n",
        "    elif choice == 8:\n",
        "      self.move(x=0, y=0)\n",
        "\n",
        "  def move(self, x=False, y=False):\n",
        "\n",
        "      # If no value for x, move randomly\n",
        "      if not x:\n",
        "          self.x += np.random.randint(-1, 2)\n",
        "      else:\n",
        "          self.x += x\n",
        "\n",
        "      # If no value for y, move randomly\n",
        "      if not y:\n",
        "          self.y += np.random.randint(-1, 2)\n",
        "      else:\n",
        "          self.y += y\n",
        "\n",
        "      # If we are out of bounds, fix!\n",
        "      if self.x < 0:\n",
        "          self.x = 0\n",
        "      elif self.x > self.size-1:\n",
        "          self.x = self.size-1\n",
        "      if self.y < 0:\n",
        "          self.y = 0\n",
        "      elif self.y > self.size-1:\n",
        "          self.y = self.size-1"
      ],
      "metadata": {
        "id": "J4lqFpzfJCA3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class to initialise the environment\n",
        "class BlobEnv:\n",
        "  SIZE = 10 # grid size for environment\n",
        "  RETURN_IMAGES = True # get images of the environment\n",
        "  MOVE_PENALTY = 1 # keep moving\n",
        "  ENEMY_PENALTY = 300 # hit the enemy\n",
        "  FOOD_REWARD = 25 # got to the food\n",
        "  OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # grid space with combinations of (x,y) for 3- colour channels\n",
        "  ACTION_SPACE_SIZE = 9 # number of actions that can be taken\n",
        "  PLAYER_N = 1  # player key in dict\n",
        "  FOOD_N = 2  # food key in dict\n",
        "  ENEMY_N = 3  # enemy key in dict\n",
        "  d = {1: (255, 175, 0), # blue player\n",
        "        2: (0, 255, 0), # green food\n",
        "        3: (0, 0, 255)} # red enemy\n",
        "\n",
        "  def reset(self):\n",
        "    '''\n",
        "    Reset the environment with new co-ordinates\n",
        "    '''\n",
        "    self.player = Blob(self.SIZE)\n",
        "    self.food = Blob(self.SIZE)\n",
        "\n",
        "    # Trying to avoid same co-ordinates for food,player,enemy\n",
        "    while self.food == self.player:\n",
        "      self.food = Blob(self.SIZE)\n",
        "    self.enemy = Blob(self.SIZE)\n",
        "    while self.enemy == self.player or self.enemy == self.food:\n",
        "      self.enemy = Blob(self.SIZE)\n",
        "\n",
        "    self.episode_step = 0\n",
        "\n",
        "    if self.RETURN_IMAGES:\n",
        "        observation = np.array(self.get_image())\n",
        "    else:\n",
        "        observation = (self.player-self.food) + (self.player-self.enemy)\n",
        "    return observation\n",
        "\n",
        "  def step(self, action):\n",
        "    '''\n",
        "    Take action for a particular step and extarct new observation, reward/penalty\n",
        "    and whether environment is done or not\n",
        "    '''\n",
        "    self.episode_step += 1\n",
        "    self.player.action(action)\n",
        "\n",
        "    #### MAYBE ###\n",
        "    #enemy.move()\n",
        "    #food.move()\n",
        "    ##############\n",
        "\n",
        "    if self.RETURN_IMAGES:\n",
        "      new_observation = np.array(self.get_image())\n",
        "    else:\n",
        "      new_observation = (self.player-self.food) + (self.player-self.enemy)\n",
        "\n",
        "    if self.player == self.enemy:\n",
        "      reward = -self.ENEMY_PENALTY\n",
        "    elif self.player == self.food:\n",
        "      reward = self.FOOD_REWARD\n",
        "    else:\n",
        "      reward = -self.MOVE_PENALTY\n",
        "\n",
        "    done = False\n",
        "    if reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or self.episode_step >= 200:\n",
        "      done = True\n",
        "\n",
        "    return new_observation, reward, done\n",
        "\n",
        "  def render(self):\n",
        "    '''\n",
        "    Renders the environment for us\n",
        "    '''\n",
        "    img = self.get_image()\n",
        "    img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.\n",
        "    cv2.imshow(\"image\", np.array(img))  # show it!\n",
        "    cv2.waitKey(1)\n",
        "\n",
        "  # FOR CNN #\n",
        "  def get_image(self):\n",
        "    '''\n",
        "    Generates input for the CNN as an image\n",
        "    '''\n",
        "    env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
        "    env[self.food.x][self.food.y] = self.d[self.FOOD_N]  # sets the food location tile to green color\n",
        "    env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]  # sets the enemy location to red\n",
        "    env[self.player.x][self.player.y] = self.d[self.PLAYER_N]  # sets the player tile to blue\n",
        "    img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
        "    return img\n",
        "\n",
        "\n",
        "env = BlobEnv() # create the environment"
      ],
      "metadata": {
        "id": "dFLeuqjWK7tF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For stats\n",
        "ep_rewards = [-200]\n",
        "\n",
        "# For more repetitive results\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "# Create models folder\n",
        "if not os.path.isdir('models'):\n",
        "  os.makedirs('models')"
      ],
      "metadata": {
        "id": "CGSsh6wVRYpY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent class\n",
        "class DQNAgent:\n",
        "  def __init__(self):\n",
        "    # Main model used for fit\n",
        "    self.model = self.create_model()\n",
        "\n",
        "    # Target network used for predict\n",
        "    self.target_model = self.create_model()\n",
        "    self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    # An array with last n steps for training - prevent over-fit on one sample\n",
        "    self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "\n",
        "    # Used to count when to update target network with main network's weights\n",
        "    self.target_update_counter = 0\n",
        "\n",
        "  def create_model(self):\n",
        "    '''\n",
        "    Create a CNN\n",
        "    '''\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(256, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))  # OBSERVATION_SPACE_VALUES = (10, 10, 3) a 10x10 RGB image.\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(256, (3, 3)))\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "    model.add(tf.keras.layers.Dense(64))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(env.ACTION_SPACE_SIZE, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (9)\n",
        "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  # Adds step's data to a memory replay array\n",
        "  # (observation space, action, reward, new observation space, done)\n",
        "  def update_replay_memory(self, transition):\n",
        "    self.replay_memory.append(transition)\n",
        "\n",
        "  # Trains main network every step during episode\n",
        "  def train(self, terminal_state, step):\n",
        "\n",
        "    # Start training only if certain number of samples is already saved\n",
        "    if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "      return\n",
        "\n",
        "    # Get a minibatch of random samples from memory replay table\n",
        "    minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "\n",
        "    # Get current states from minibatch, then query NN model for Q values\n",
        "    current_states = np.array([transition[0] for transition in minibatch])/255\n",
        "    current_qs_list = self.model.predict(current_states)\n",
        "\n",
        "    # Get future states from minibatch, then query NN model for Q values\n",
        "    # When using target network, query it, otherwise main network should be queried\n",
        "    new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
        "    future_qs_list = self.target_model.predict(new_current_states)\n",
        "\n",
        "    X = [] # input features\n",
        "    y = [] # target\n",
        "\n",
        "    # Now we need to enumerate our batches\n",
        "    for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "      # If not a terminal state, get new q from future states, otherwise set it to 0\n",
        "      # almost like with Q Learning, but we use just part of equation here\n",
        "      if not done:\n",
        "        max_future_q = np.max(future_qs_list[index])\n",
        "        new_q = reward + DISCOUNT * max_future_q\n",
        "      else:\n",
        "        new_q = reward\n",
        "\n",
        "      # Update Q value for given state\n",
        "      current_qs = current_qs_list[index]\n",
        "      current_qs[action] = new_q\n",
        "\n",
        "      # And append to our training data\n",
        "      X.append(current_state)\n",
        "      y.append(current_qs)\n",
        "\n",
        "    # Fit on all samples as one batch, log only on terminal state\n",
        "    self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False if terminal_state else None)\n",
        "\n",
        "    # Update target network counter every episode\n",
        "    if terminal_state:\n",
        "      self.target_update_counter += 1\n",
        "\n",
        "    # If counter reaches set value, update target network with weights of main network\n",
        "    if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "      self.target_model.set_weights(self.model.get_weights())\n",
        "      self.target_update_counter = 0\n",
        "\n",
        "  # Queries main network for Q values given current observation space (environment state)\n",
        "  def get_qs(self, state):\n",
        "    return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
        "\n",
        "\n",
        "agent = DQNAgent()"
      ],
      "metadata": {
        "id": "VuOw0SdtSjs6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over episodes\n",
        "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
        "\n",
        "  # Restarting episode - reset episode reward and step number\n",
        "  episode_reward = 0\n",
        "  step = 1\n",
        "\n",
        "  # Reset environment and get initial state\n",
        "  current_state = env.reset()\n",
        "\n",
        "  # Reset flag and start iterating until episode ends\n",
        "  done = False\n",
        "  while not done:\n",
        "    if np.random.random() > epsilon:\n",
        "        # Get action from Q table\n",
        "        action = np.argmax(agent.get_qs(current_state))\n",
        "    else:\n",
        "        # Get random action\n",
        "        action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
        "\n",
        "    new_state, reward, done = env.step(action)\n",
        "\n",
        "    # Transform new continous state to new discrete state and count reward\n",
        "    episode_reward += reward\n",
        "\n",
        "    if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
        "      env.render()\n",
        "\n",
        "    # Every step we update replay memory and train main network\n",
        "    agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
        "    agent.train(done, step)\n",
        "\n",
        "    current_state = new_state\n",
        "    step += 1\n",
        "\n",
        "  # Append episode reward to a list and log stats (every given number of episodes)\n",
        "  ep_rewards.append(episode_reward)\n",
        "  if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
        "    average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "    min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "    max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "\n",
        "    # Save model, but only when min reward is greater or equal a set value\n",
        "    if min_reward >= MIN_REWARD:\n",
        "        agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
        "\n",
        "  # Decay epsilon\n",
        "  if epsilon > MIN_EPSILON:\n",
        "    epsilon *= EPSILON_DECAY\n",
        "    epsilon = max(MIN_EPSILON, epsilon)"
      ],
      "metadata": {
        "id": "yLv7tQYGaBNS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}